# -*- coding: utf-8 -*-
"""AI_Buddy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uoy_zlMJuYg0KASKIusYtTaRqmG6wMUh
"""

!pip install langchain-community

!pip install pypdf

!pip install chromadb

!pip install langchain-text-splitters

!pip install tiktoken

!pip install transformers

!pip install sentence_transformers

"""Submodule from langchain to load pdf file"""

from langchain.document_loaders import PyPDFLoader

"""Submodule to split text from the loaded pdf"""

from langchain.text_splitter import RecursiveCharacterTextSplitter

"""Submodule from langchain to convert text embeddings"""

from langchain.embeddings import HuggingFaceEmbeddings

"""To store the embeddings"""

from langchain.vectorstores import Chroma

"""To connect the hugging face models

"""

from transformers import pipeline

"""Load the pdf file"""

pdf_path = "Ai Buddy.pdf"   #pdf location (path)
data = PyPDFLoader(pdf_path).load()  #loading data

"""Displaying the data of pdf"""

data

persist_dir = "/content/vector_db"

"""Setting the directory for storing the vector database"""

from google.colab import drive
drive.mount('/content/drive')

"""Splitting the data into chunks"""

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

"""Splitting on the basis of condition above"""

splits = text_splitter.split_documents(data)

splits[5]

"""Initialize embeddings using hugging face model (sen"""

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

"""Initialize vector database"""

vector_db = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=persist_dir)

"""Initailizing the Roberta-based question answering pipeline from Hugging face"""

qa_pipeline = pipeline("question-answering",model="deepset/roberta-base-squad2")

"""Initializing the Retrieval with Multi-Query Ability"""

retriever = vector_db.as_retriever()

"""Function to Execute a Query with context Retrieved"""

def execute_query(question):
  retreived_docs = retriever.get_relevant_documents(question)

  #check if any Document are Retreived
  if not retreived_docs:
    return "No Relevant Documents Found"

  #Prepare Context from the Retrieved Document
  context = " ".join([doc.page_content.strip() for doc in retreived_docs])
  context = " ".join(context.split())  #Ensure it is in proper format
  print(f"Retrieved Context:\n{context}\n")

  #Use the qa pipeline to answer the Questions based on context
  response = qa_pipeline({
      "context": context,
      "question": question
  })

  #Check the model return an answer
  if not response or "answer" not in response:
    print("No valid Answer Generated bt the Model")
    return "NO VALID ANSWER GENERATED !"

  return response["answer"]

"""Testing Model with Simple Question"""

response = execute_query("What is AI Buddy?")

